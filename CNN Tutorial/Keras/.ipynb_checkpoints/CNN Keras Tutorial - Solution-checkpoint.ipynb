{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks with Keras\n",
    "***\n",
    "\n",
    "![keras](../keras_logo.png)\n",
    "\n",
    "***\n",
    "__Keras is a high level machine learning API built using Python. The library is is running on top of Tensorflow, Theano or CNTK which allows you the choice of what framework backend you want to run your model with.__ \n",
    "\n",
    "Throughout each of the tutorials on convolutional neural networks I will be referencing the associated theory document so that you can get a good understanding of what is going on in the background. So, please make sure to read the theory that I reference if you are struggling to understand what is going on. \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Convolution2D, Dense, Dropout, Flatten, MaxPooling2D\n",
    "from keras.optimizers import Adam, SGD, rmsprop\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Load In The Data\n",
    "You will need to download the data from [Kaggle](https://www.kaggle.com/c/dogs-vs-cats/data). The data has been organised into a training and testing folder. Before we can get to work with the data it needs to be organised into folders representing all of the cats and all of the dogs. It's very important that you are able to understand the data that you are working with before building and training the model.\n",
    "***\n",
    "In Jupyter Notebooks you can interact directly with the console by using the \"!\" syntax before your command. It is useful to make use of this to interact with your data directly and find the location of specific files without having to leave the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../../../data/all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with file paths a lot, it is good to define them beforehand. We have done that here; a general path, which directs you to all of you downloaded data and a train/test path that points directly at our image data that we want to use. Now we just have to call their set variable names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"../../../all/\"\n",
    "TRAIN_PATH = \"../../../data/all/train/\"\n",
    "TEST_PATH = \"../../../data/all/test1/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set the image height and width that we want all of our images to be. Decreasing the size of our images before we feed them through the model decreases the time it takes to train as there is less data to process. \n",
    "\n",
    "```python\n",
    "IMG_HEIGHT = 256\n",
    "IMG_WIDTH = 256\n",
    "```\n",
    "We then create two arrays which will store our images and their associated class or label, which is found by looking at the file name. \n",
    "\n",
    "```python\n",
    "images = []\n",
    "classes = []\n",
    "```\n",
    "The for loop goes through every image in the training folder and loads each image in individually, appending each to the \"images\" array. Here we can use the height and width that we set. We then check the filename of the image file that we just loaded in for either cat or dog to set the according number. \n",
    "\n",
    "```python\n",
    "for filename in tqdm(os.listdir(TRAIN_2_PATH)):\n",
    "    img = mpimg.imread(TRAIN_2_PATH+filename)\n",
    "    images.append(cv.resize(img, (IMG_HEIGHT, IMG_WIDTH)))\n",
    "    if \"cat\" in filename:\n",
    "        classes.append(0)\n",
    "    elif \"dog\" in filename:\n",
    "        classes.append(1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT = 256\n",
    "IMG_WIDTH = 256\n",
    "\n",
    "images = []\n",
    "classes = []\n",
    "\n",
    "for filename in tqdm(os.listdir(TRAIN_PATH)):\n",
    "    img = mpimg.imread(TRAIN_PATH+filename)\n",
    "    images.append(cv.resize(img, (IMG_HEIGHT, IMG_WIDTH)))\n",
    "    if \"cat\" in filename:\n",
    "        classes.append(0)\n",
    "    elif \"dog\" in filename:\n",
    "        classes.append(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure that we have loaded our images in correctly and our labels for the associated image are correct we can plot the images with their label. We will do this with matplotlib and plot the first six images with their labels as the title. Make sure that the labels are correctly matching the image, 1 for dogs and 0 for cats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "COL = 3\n",
    "ROW = 2\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "for i in range(0, COL*ROW):\n",
    "    fig.add_subplot(ROW, COL, i+1)          #IF YOU DON'T HAVE +1, IT FALLS OUT OF RANGE\n",
    "    plt.title(str(classes[i]))\n",
    "    plt.imshow(images[i])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment The Data\n",
    "It is important that we augment (manipulate) our data to generalise and avoid overfitting. Overfitting happens when our model becomes to acustomed to the features of our training data and so performs worse when validating our performance. By augmenting our data we are generating a set amount of variations for each image that we pass into the `ImageDataGenerator()`. \n",
    "***\n",
    "Set X equal to the \"images\" array wrapped in another numpy array; we need to do this because the model is expecting a 4 dimensional array. The array it is expecting should look like, `(Total Images, Img Height, Img Width, Img Depth)` when using the `.shape()` function on the array. Y is set equal to the \"classes\" array as the classes array needs to be in an array of the same dimensions as X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(images, dtype=np.float32)\n",
    "Y = np.array(classes, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the scikit-learn library which has a lot of useful functions that we can make use of, we are going to use the `train_test_split` function in particular. The train_test_split function allows us to take our total data to create a training set and a validation set. The training and validation data allows us to see how the model is performing while it is training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ImageDataGenerator()` object gives us the ability to generate variations of our images so that we can generalise our model by introducing more variations on the images that it is looking at and training on. When setting the parameters of the `ImageDataGenerator`, we are selecting the range of how much we want to affect the generated variations of the selected image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1. / 255, \n",
    "                                   rotation_range=40, \n",
    "                                   #width_shift_range=60, \n",
    "                                   #height_shift_range=30, \n",
    "                                   #brightness_range=10, \n",
    "                                   shear_range=10.0, \n",
    "                                   #zoom_range=5.0, \n",
    "                                   horizontal_flip=True)\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1. / 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.flow()` function is used to generate batches of data using the images and labels. Here we will be generating batch sizes of 125 images and labels. `shuffle` when equal to `True` shuffles the data so that it is not in the set order that it has been downloaded in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow(x_train, y_train, batch_size=125, shuffle=True)\n",
    "valid_generator = valid_datagen.flow(x_val, y_val, batch_size=125)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build The Model\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image_processing](../image_input.png)\n",
    "***\n",
    "\n",
    "Now that we have collected our data, modified the images and augmented them to increase the reliability of our model. We need to build the model that we will be training on the data that we have prepared. We will build a `Sequential()` model as it is capable for most problems, the alternative is a functional model which allows for a lot more flexibility as you can connect to more than just the previous and next layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Convolution2D` layer extracts the features from the data passed in. Imagine we have the 5x5 image (green) and that it is a special case where the the pixels are only 1 or 0. We will take the 3x3 matrix and slide it over the original image by 1 pixel at a time (this is our stride) and multiply the elements and add them to get a final value for the feature map.\n",
    "\n",
    "![image_processing](../ConvolvedFeat.gif)\n",
    "\n",
    "ReLU (Rectified Linear Unit) steps are normally introduced after convolution operations in CNN’s. ReLU is applied per pixel and replaces any negative values with a 0 value.\n",
    "\n",
    "![image_processing](../ReluActivation.png)\n",
    "\n",
    "The `MaxPooling2D` layer reduces the dimensionality of the feature maps while retaining the important information. We perform pooling to redact the spatial size of the input, this means pooling makes the input representations smaller, reduces network computations which reduces overfitting and makes the network invariant to small transformations or distortions. \n",
    "\n",
    "![image_processing](../Pooling.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Convolution2D(128, (5, 5), input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Convolution2D(128, (5, 5)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\"))\n",
    "\n",
    "model.add(Convolution2D(64, (3, 3)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below is what is known as a fully connected layer. The fully connected layer implies that every neutron in the previous layer is connected to every neutron in the next layer. The most popular activation function in the output layer is Softmax however other classifiers such as SVM can be used. \n",
    "\n",
    "![image_processing](../FullyConnected.png)\n",
    "\n",
    "The concolution layers above represent the high level features of the data, while adding a fully connected layer is a cheap method of learning a non-linear combination of these features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(256))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation(\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", metrics=[\"accuracy\"], loss=\"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(train_generator, \n",
    "                    steps_per_epoch=20000 // 125, \n",
    "                    epochs=1, \n",
    "                    validation_data=valid_generator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
