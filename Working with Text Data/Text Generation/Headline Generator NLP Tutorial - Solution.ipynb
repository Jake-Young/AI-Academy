{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Headline Generation (NLP)\n",
    "***\n",
    "![Newspaper](Images/HeadlineImage.jpg)\n",
    "***\n",
    "In this tutorial we are going to use a recurrent neural network to look at headlines from newspapers, to then be able to generate new headlines based on the seed (first few words) that we give it. The data we have given you contains only American headlines, so it will be biased. We suggest trying your own data if you find something similar!\n",
    "***\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "from numpy.random import seed\n",
    "set_random_seed(2)\n",
    "seed(1)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "Here we have data define the exact file we want to load in as we only want the data that is associated with articles and not the comments.\n",
    "\n",
    "To Do:\n",
    "- Load all **article** data\n",
    "- Take a look at the headlines that we have loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mArticlesApril2017.csv\u001b[m\u001b[m \u001b[31mArticlesMarch2017.csv\u001b[m\u001b[m \u001b[31mCommentsFeb2018.csv\u001b[m\u001b[m\r\n",
      "\u001b[31mArticlesApril2018.csv\u001b[m\u001b[m \u001b[31mArticlesMarch2018.csv\u001b[m\u001b[m \u001b[31mCommentsJan2017.csv\u001b[m\u001b[m\r\n",
      "\u001b[31mArticlesFeb2017.csv\u001b[m\u001b[m   \u001b[31mArticlesMay2017.csv\u001b[m\u001b[m   \u001b[31mCommentsJan2018.csv\u001b[m\u001b[m\r\n",
      "\u001b[31mArticlesFeb2018.csv\u001b[m\u001b[m   \u001b[31mCommentsApril2017.csv\u001b[m\u001b[m \u001b[31mCommentsMarch2017.csv\u001b[m\u001b[m\r\n",
      "\u001b[31mArticlesJan2017.csv\u001b[m\u001b[m   \u001b[31mCommentsApril2018.csv\u001b[m\u001b[m \u001b[31mCommentsMarch2018.csv\u001b[m\u001b[m\r\n",
      "\u001b[31mArticlesJan2018.csv\u001b[m\u001b[m   \u001b[31mCommentsFeb2017.csv\u001b[m\u001b[m   \u001b[31mCommentsMay2017.csv\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['N.F.L. vs. Politics Has Been Battle All Season Long',\n",
       " 'Voice. Vice. Veracity.',\n",
       " 'A Stand-Up\\xe2\\x80\\x99s Downward Slide',\n",
       " 'New York Today: A Groundhog Has Her Day',\n",
       " 'A Swimmer\\xe2\\x80\\x99s Communion With the Ocean',\n",
       " 'Trail Activity',\n",
       " 'Super Bowl',\n",
       " 'Trump\\xe2\\x80\\x99s Mexican Shakedown',\n",
       " 'Pence\\xe2\\x80\\x99s Presidential Pet',\n",
       " 'Fruit of a Poison Tree']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_dir = 'data/'\n",
    "all_headlines = []\n",
    "for filename in os.listdir(curr_dir):\n",
    "    if 'Articles' in filename:\n",
    "        article_df = pd.read_csv(curr_dir + filename)\n",
    "        all_headlines.extend(list(article_df.headline.values))\n",
    "        break\n",
    "\n",
    "all_headlines[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the Data\n",
    "We need to clean the text of the data because it appears to be in unicode which is why we get apostrophe's appearing like \"\\xe2\\x80\\x99\". We will define a function where we can pass all of our text through and which returns the text without any punctuation and capitol letters. \n",
    "\n",
    "To Do:\n",
    "- Write a function which checks for punctuation, removes it and changes all letters to lower case\n",
    "- Pass your data through the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(txt):\n",
    "    txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
    "    txt = txt.decode(\"ascii\", \"ignore\")\n",
    "    return txt.encode(\"utf8\") \n",
    "\n",
    "corpus = [clean_text(x) for x in all_headlines]\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenise\n",
    "With the Twitter Classification we tokenised our words, we will do the same here and create a bag of words. A bag of words is something which counts the amount of occurences of given word and labels it with a unique identifier. \n",
    "\n",
    "To Do:\n",
    "- Define a function to get a sequence of tokens\n",
    "- Define a function to generate padded sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "def get_sequence_of_tokens(corpus):\n",
    "    ## tokenisation\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index)+1\n",
    "    \n",
    "    ## convert data to sequence of tokens\n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    return input_sequences, total_words\n",
    "\n",
    "input_sequences, total_words = get_sequence_of_tokens(corpus)\n",
    "input_sequences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padded_sequences(input_sequences):\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "    \n",
    "    predictors, label = input_sequences[:,:-1], input_sequences[:,-1]\n",
    "    label = ku.to_categorical(label, num_classes=total_words)\n",
    "    return predictors, label, max_sequence_len\n",
    "\n",
    "predictors, label, max_sequence_len = generate_padded_sequences(input_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Model and Train\n",
    "We're going to create a model using a Long Short Term Memory (LSTM) layer. Traditional neural networks usually throw away what they've learned previously and start over again. Recurrent Nueral Networks (RNN) are different, what they learn persists through each layer. A typical RNN can struggle to identify and learn the long term dependecies of the data. This is where a LSTM comes in as it is capable of learning long term dependencies. \n",
    "\n",
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "To Do:\n",
    "- Create a model with the following layers:\n",
    "            Embedding\n",
    "            LSTM\n",
    "            Dense\n",
    "- Compile the model\n",
    "- Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(max_sequence_len, total_words):\n",
    "    input_len = max_sequence_len - 1\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add input embedding layer\n",
    "    model.add(Embedding(total_words, 10, input_length=input_len))\n",
    "    \n",
    "    # Add hidden layer 1 - LSTM layer\n",
    "    model.add(LSTM(100))\n",
    "    \n",
    "    # Add output layer \n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model(max_sequence_len, total_words)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(predictors, label, epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate your Headlines\n",
    "To generate the headlines we are going to create a function which takes in the beggining of our headline (the topic), how long you want the headline to be, the model we created and how long we want our sequences to be. \n",
    "\n",
    "To Do:\n",
    "- Create a generate_text function\n",
    "- Generate different headlines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted = model.predict_classes(token_list, verbose=0)\n",
    "        \n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \"+output_word\n",
    "    return seed_text.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (generate_text(\"united states\", 5, model, max_sequence_len))\n",
    "print (generate_text(\"preident trump\", 4, model, max_sequence_len))\n",
    "print (generate_text(\"donald trump\", 4, model, max_sequence_len))\n",
    "print (generate_text(\"india and china\", 4, model, max_sequence_len))\n",
    "print (generate_text(\"new york\", 4, model, max_sequence_len))\n",
    "print (generate_text(\"science and technology\", 5, model, max_sequence_len))\n",
    "print (generate_text(\"fake news\", 5, model, max_sequence_len))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
